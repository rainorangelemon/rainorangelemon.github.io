
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Motion Planning with GNN</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://rainorangelemon.github.io/images/2021.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://rainorangelemon.github.io/NeurIPS2021"/>
    <meta property="og:title" content="Motion Planning with GNN" />
    <meta property="og:description" content="Project page for Motion Planning with GNN." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Motion Planning with GNN" />
    <meta name="twitter:description" content="Project page for Motion Planning with GNN." />
    <meta name="twitter:image" content="https://rainorangelemon.github.io/images/2021.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="manifest" href="site.webmanifest">    

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Reducing Collision Checking for Sampling-Based Motion Planning
            </br> Using Graph Neural Networks
                <small>
                </br>NeurIPS 2021
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://rainorangelemon.github.io">
                          Chenning Yu
                        </a>
                    </li>
                    <li>
                        <a href="https://scungao.github.io/">
                            Sicun Gao
                        </a>
                    </li>
                </br>University of California, San Diego
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">  
                    <!-- original was 4 and 2 -->
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://rainorangelemon.github.io/NeurIPS2021/paper.pdf">
                                <strong>paper</strong>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/rainorangelemon/gnn-motion-planning">
                                <strong>code</strong>
                            </a>
                        </li>                        
                        <li>
                            <a href="https://recorder-v3.slideslive.com/#/share?share=49693&s=d9031a8a-7e3c-4742-91da-d4a1298b1c5e">
                                <strong>presentation</strong>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://github.com/">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>

        <div class="row" >
        <!-- <image src="img/cc-1.png" class="img-responsive" alt="overview"><br> -->
        </div>

        <div class="row" >
            <div class="col-md-8 col-md-offset-2" style="background-color: #F7F6F4;">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Sampling-based motion planning is a popular approach in robotics for finding paths in continuous configuration spaces. Checking collision with obstacles is the major computational bottleneck in this process. We propose new learning-based methods for reducing collision checking to accelerate motion planning by training graph neural networks (GNNs) that perform path exploration and path smoothing. Given random geometric graphs (RGGs) generated from batch sampling, the path exploration component iteratively predicts collision-free edges to prioritize their exploration. The path smoothing component then optimizes paths obtained from the exploration stage. The methods benefit from the ability of GNNs of capturing geometric patterns from RGGs through batch sampling and generalize better to unseen environments. Experimental results show that the learned components can significantly reduce collision checking and improve overall planning efficiency in challenging high-dimensional motion planning tasks.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overall Algorithm
                </h3>
                <p style="text-align:center;">
                    <image style="width: 90%; height: 90%" src="img/framework.png" class="img-responsive">
                </p>
            </div>
        </div>        


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Graph Neural Network Architecture
                </h3>
                <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
                    <image style="width: 100%; height: 100%" src="img/Architecture.png" class="img-responsive">
                </p>
                </div>
            </div>
        </div>
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>               
                <div class="col-md-8 col-md-offset-2">
                    <p style="text-align:center;">
                    <image src="img/envs_2-1.png" style="width: 100%; height: 100%" class="img-responsive">
                    </p>
                </div>
                </br>
                <p style="text-align:center;">
                    <image src="img/overall_0526-1.png" height="10px" class="img-responsive">
                </p>

            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is not capable of learning to represent the same scene at multiple levels of detail, with blurring in close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details in close-ups and correctly renders the zoomed-out images.
                </p>                
                <br>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/ship_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/chair_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/lego_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/mic_sbs_path1.mp4" type="video/mp4" />
                </video>
                <br><br>
                <p class="text-justify">
                    We can also manipulate the integrated positional encoding by using a larger or smaller radius than the true pixel footprint, exposing the continuous level of detail learned within a single network:
                </p>     
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/lego_radii_manip_slider_200p.mp4" type="video/mp4" />
                </video>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Videos
                </h3>
                <div class="text-center">
                    <div style="position:relative;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/videoseries?list=PLkf-TgOMfznLVgyRkJIDILl5S9CksG9JR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>                
                    </div>
                </div>
            </div>
        </div>        


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">Wikipedia</a> provides an excellent introduction to spatial anti-aliasing techniques.
                </p>
                <p class="text-justify">
                    Mipmaps were introduced by Lance Williams in his paper "Pyramidal Parametrics" (<a href="https://software.intel.com/sites/default/files/m/7/2/c/p1-williams.pdf">Williams (1983)</a>).
                </p>
                <p class="text-justify">
                    <a href="https://dl.acm.org/doi/abs/10.1145/964965.808589">Amanatides (1984)</a> first proposed the idea of replacing rays with cones in computer graphics rendering. 
                </p>
                <p class="text-justify">
                    The closely related concept of <em>ray differentials</em> (<a href="https://graphics.stanford.edu/papers/trd/">Igehy (1999)</a>) is used in most modern renderers to antialias textures and other material buffers during ray tracing.
                </p>
                <p class="text-justify">
                    Cone tracing has been used along with prefiltered voxel-based representations of scene geometry for speeding up indirect illumination calculations in <a href="https://research.nvidia.com/sites/default/files/publications/GIVoxels-pg2011-authors.pdf">Crassin et al. (2011)</a>.
                </p>
                <p class="text-justify">
                    Mip-NeRF was implemented on top of the <a href="https://github.com/google-research/google-research/tree/master/jaxnerf">JAXNeRF</a> codebase.
                </p>
            </div>
        </div> -->
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2" style="background-color: #F7F6F4;">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <pre style="font-size:11px; background-color: #E5E3DD">
@inproceedings{chenning2021collision,
    title={Reducing Collision Checking for Sampling-Based Motion Planning Using Graph Neural Networks},
    author={Yu, Chenning and Gao, Sicun},
    booktitle={Proceedings of the 35rd International Conference on Neural Information Processing Systems},
    year={2021}
}
</pre>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2" style="background-color: #F7F6F4;">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://jonbarron.info/mipnerf/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
